---
title: "ST340 Lab 4: Expectation Maximization"
date: "2020--21"
output: pdf_document
---
# 1: Expectation Maximization---mixture of Gaussians

```{r}
  library(mvtnorm)
```

(a) Define parameters for $K = 3$ multivariate normal distributions.
```{r}
K <- 3
mus.actual <- matrix(0,3,2)
mus.actual[1,] <- c(0,0)
mus.actual[2,] <- c(4,4)
mus.actual[3,] <- c(-4,4)
```

(b) Generate the covariance matrices randomly.
```{r}
Sigmas.actual <- list()
for (k in 1:K) {
  mtx <- matrix(1,2,2)
  mtx[1,2] <- runif(1)*2-1
  mtx[2,1] <- mtx[1,2]
  Sigmas.actual[[k]] <- mtx*exp(rnorm(1))
}
```

(c) Generate some random mixture weights.
```{r}
ws <- runif(K)
ws <- ws/sum(ws)
```

(d) Generate 1000 data points in $\mathbb{R}^2$. Hint: look at `?rmvnorm`.
```{r}
n <- 1000
p <- 2
xs <- matrix(0,n,p)
cols <- rep(0,n)
for (i in 1:n) {
  # sample from the mixture by sampling a mixture component k...
  k = sample(1:3,1, prob = ws)
  
  # ...and then sampling from that mixture component
  xs[i,] = rmvnorm(1, mean = mus.actual[k,], sigma = Sigmas.actual[[k]])
  
  
  cols[i] <- k
}
```

(e) Plot the data points coloured by cluster.
```{r}
plot(xs,col=cols,pch=20)    #plotting generated data points from random MVN samples and colour coded based on distribution used
```

(f) Plot the data points without the colours.
```{r}
plot(xs,pch=20)
```

(g) Run the EM algorithm on your generated data. You can try seeing what happens if $K=2$ or $K=4$ as well$\dots$
```{r}
source("data/em_mixture_gaussians.R")
print(system.time(out <- em_mix_gaussian(xs,K=3)))

my.colors <- rep(0,n)
for (i in 1:n) {
  my.colors[i] <- rgb(out$gammas[i,1],out$gammas[i,2],out$gammas[i,3])
}

# recall that gamma_i_k is the probability that xi is associated with cluster k

plot(xs,col=my.colors,pch=20)
```

# 2: Expectation Maximization---mixture of Bernoullis

(a) Create a file called `em_mixture_bernoullis.R` which contains a function called `em_mix_bernoulli` that is the analogue of `em_mix_gaussian`. You could use `em_mixture_gaussians.R` as a template.

Hint: do not initialize any of the cluster `mu`s to be either 0 or 1. (Do you know why?)
\
As otherwise if for example $\mu_{kj}$ is equal to one and then $x_{ij}$ is zero we will have zero probability for $p(\boldsymbol{x}_i|\boldsymbol{\mu}_{k})$. Which is undefined when taking the log.

Hint: if, by numerical error, any of the parameters $\mu_{kj}$ are greater than 1, the algorithm will most likely fail. To avoid this, you can, after the $M$ step, perform the update
```{r, eval=FALSE}
mus[which(mus > 1,arr.ind=TRUE)] <- 1 - 1e-15
```
where `mus` is a $K \times p$ matrix.

(b) Test your code.
```{r}
n <- 500; p <- 50                           # n data points where each data point is a vector of length 50. We are assuming each element in these vectors is from a bernoulli dist.
K.actual <- 2                               # actual number of clusters

mix <- runif(K.actual)                      # the mixture probabilities w_k. The probabilities for selecting a certain mixture component
mix <- mix / sum(mix)                       # normalising the probs

mus.actual <- matrix(runif(K.actual*p),K.actual,p)    # matrix 2 rows 50 columns. This is selecting our parameters mu for each vector. Each vector is made up of 50 mus.
zs.actual <- rep(0,n)                       # actual cluster assignment for each of the 500 data points

xs <- matrix(0,n,p)                         # no we are simulating a dataset
for (i in 1:n) {
  cl <- sample(K.actual,size=1,prob=mix)    # important to note that: If x has length 1, is numeric and x >= 1, sampling via sample takes place from 1:x.
                                            # selects cluster 1 or 2
  
  zs.actual[i] <- cl                        # zs.actual takes the cluster value
  
  xs[i,] <- (runif(p) < mus.actual[cl,])    # this is used to simulate our n data points. A large value in mus.actual is more likely to give a TRUE (1) value as there is a higher prob that the random sample                                                  from the uniform dist will be less than this value.
                                            
}
```

(c) Calculate the mixture parameters.
```{r}
source("data/em_mixture_bernoullis.R")
source('my bernoulli EM function.R')
print(system.time(out <- em_mix_bernoulli(xs,K.actual)))
```

(d) Check if the learned parameters are close to the truth.
```{r}
v1 <- sum(abs(out$mus-mus.actual))
v2 <- sum(abs(out$mus[2:1,]-mus.actual))
vm <- min(v1,v2)/p/2
print(vm)
if (vm > .3) print("probably not working") else print("might be working")
```


```{r}
# Checking if my algorithm gives the same results

out = my_em_mix_bernoulli(xs,K = K.actual)

v1 <- sum(abs(out$Mu-mus.actual))
v2 <- sum(abs(out$Mu[2:1,]-mus.actual))
vm <- min(v1,v2)/p/2
print(vm)
if (vm > .3) print("probably not working") else print("might be working")

```











