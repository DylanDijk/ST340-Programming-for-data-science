---
title: "ST340 Assignment 2"
author: "Dylan Dijk (1802183), Kum Mew Lee, Aryan Gupta"
date: "27/02/2021"
output:
  pdf_document: 
    toc: true
    toc_depth: 3
  html_document: default
  word_document: default
---
```{r libraries, include=FALSE}
library(doBy)
library(kableExtra)
library(gtools)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = ">")
load('data/20newsgroups.rdata')    # Loading in data

source('data/em_mixture_bernoullis.R')   # Loading in EM function for bernoulli mixtures from week 5 lab 4
```


\newpage

# 1

## A

In the M-step $II$ we want to maximize $f(\boldsymbol{\mu_{1:k}}) = \sum_{i = 1}^{n} \sum_{k = 1}^{K}\gamma_{ik}\log(p(\boldsymbol{x_i}|{\boldsymbol{\mu_k}}))$
\
And we know that $p(\boldsymbol{x_i}|\boldsymbol{\mu_k}) = \displaystyle\prod_{j = 1}^{p} \mu_{kj}^{x_{ij}}(1 - \mu_{kj})^{(1 - x_{ij})}$
\
\
Therefore we want to maximize:
\[f(\boldsymbol{\mu_{1:k}}) = \sum_{i = 1}^{n} \sum_{k = 1}^{K}\gamma_{ik} \sum_{j = 1}^{p}  [x_{ij}\log(\mu_{kj}) + (1-x_{ij})\log(1-\mu_{kj})]\]

Now if we fix $k \in \{1,\dots,K\}$, we have:

\[f(\boldsymbol{\mu_k}) = \sum_{i = 1}^{n}   \gamma_{ik} \sum_{j = 1}^{p}  [x_{ij}\log(\mu_{kj}) + (1-x_{ij})\log(1-\mu_{kj})]\]

Now taking the partial derivative w.r.t a single $\mu_j$:

\[\frac{\partial f}{\partial \mu_j}  = \sum_{i = 1}^{n} \frac{\gamma_{ik}x_{ij}}{\mu_j} - \frac{\gamma_{ik}(1-x_{ij})}{1 - \mu_j} \]

Setting this equal to zero we get:

\[(1 - \mu_{j}) \sum_{i=1}^{n}\gamma_{ik}x_{j} = \mu_j \sum_{i=1}^{n}\gamma_{ik}(1 - x_{ij})      \]

After rearranging we have:

\[    \mu_j = \frac{\sum_{i=1}^{n}\gamma_{ik} x_{ij}}{\sum_{i =1}^{n}\gamma_{ik}}  \hspace{1cm} \implies \hspace{1cm} \boldsymbol{\mu_{k}} = \frac{\sum_{i=1}^{n}\gamma_{ik} \boldsymbol{x_{i}}}{\sum_{i =1}^{n}\gamma_{ik}}  \]


Therefore have shown the unique stationary point is obtained by choosing for each k
\
\[\boldsymbol{\mu_{k}} = \frac{\sum_{i=1}^{n}\gamma_{ik} \boldsymbol{x_{i}}}{\sum_{i =1}^{n}\gamma_{ik}}\]




\







## B
### i 

Here I have just used the function given to us for Lab 4.
```{r  part i running EM algorithm, eval=FALSE}
xs = documents
out_Q1bi = em_mix_bernoulli(xs, K  = 4)       
```


### ii 

The algorithm does not tell us what each cluster represents and we have to infer this ourselves.
\
Below I have made a table giving the associated words of the largest 8 values of $\mu$ in $\boldsymbol{\mu}{_k}$ for each k.


```{r, include = FALSE}
load("output_of_EM_mix_bernoulli.rds")

top8_words = data.frame('1' = wordlist[which.maxn(out_Q1bi$mus[1,], n = 8)],
                        '2' = wordlist[which.maxn(out_Q1bi$mus[2,], n = 8)],
                        '3' = wordlist[which.maxn(out_Q1bi$mus[3,], n = 8)],
                        '4' = wordlist[which.maxn(out_Q1bi$mus[4,], n = 8)])
```

```{r, echo=FALSE}

kable(top8_words, col.names = 1:4, align = 'c') %>%
  kable_styling(position = "center")
```

From this table it seems pretty clear that cluster 1 represents articles related to computers and cluster 3 represents articles to sport ('rec' group).
\
For clusters 2 and 4 it is less clear. Could say cluster 2 represents the articles from the 'science' group as "university" has a large $\mu$ and then cluster 4 represents articles from the 'talk' group.
\
So in summary my guess would be 1 = `comp.*`, 2 = `sci.*`, 3 = `rec`, 4 = `talk.*`
\

Below I have looked at all possible permutations of the 4 clusters, and calculated the difference between the $\gamma$ values of each data row and the actual label from the `newsgroups.onehot` dataset. As the $\gamma$ value represents the probability that each data point belongs to a certain cluster.
\
```{r}
perm = permutations(n = 4, r = 4, v = 1:4)   
gamma_minus_label = vector(length = 24)
for(i in 1:24){
  gamma_minus_label[i] = sum(abs(out_Q1bi$gammas[,perm[i,]] - newsgroups.onehot))
}
```
Comparing my guess for the labeling of the clusters with the order of the `groupnames` vector,

```{r}
groupnames
```
 the permutation that should give the minimum value should be `1 3 4 2`.
 
```{r}
algorithm_labels = perm[which.min(gamma_minus_label),]
algorithm_labels
```
And the average value of the difference between the actual lables and the gammas for this minimum case is:
```{r}
min(gamma_minus_label)/nrow(documents)
```


### iii

So in the function given to us for Lab 4 it uses   `.2 + .6*xs[sample(n,K),]`   to select the starting $\mu_k$ for each k. 
\
This randomly samples K rows from the dataset we are inputting into the algorithm and assigns $\mu$ equal to 0.2 if an element of the row is zero and 0.8 if the element is one.
Important to note that we shouldn't set a starting $\mu$ equal to zero or one. *The code for how I ran the following 3 cases is in the markdown file*
\

```{r, eval=FALSE, include=FALSE}
#A
out_mu_4_6 = em_mix_bernoulli_mu_4_6(xs, K  = 4)  # Changed starting mu in function to .4 + .2*xs[sample(n,K),]

#B
out_W_prop = em_mix_bernoulli_W_prop(xs, K  = 4)  
# Changed starting weights in function to
# lws <- c(log(0.2835242),log(0.2166605), log(0.1635882), log(0.3362271))
# the code for how I calculated these values is shown in the part (B) section

#C
out_W_mu_prop  =  em_mix_bernoulli_W_mu_prop(xs, K = 4)
# Used same starting weights as part B and used starting mus:
# mus <- mus_prop
# again the code for how I got mus_prop is beloe in the part (C) section
```



**(A)** I am going to rerun the algorithm but now use `.4 + .2*xs[sample(n,K),]`, this now assigns $\mu$ equal to 0.4 if an element of the row is zero and 0.6 if the element is one.
```{r, include=FALSE}
top8_words_mu_4_6 = data.frame('1' = wordlist[which.maxn(out_mu_4_6$mus[1,], n = 8)],
                               '2' = wordlist[which.maxn(out_mu_4_6$mus[2,], n = 8)],
                               '3' = wordlist[which.maxn(out_mu_4_6$mus[3,], n = 8)],
                               '4' = wordlist[which.maxn(out_mu_4_6$mus[4,], n = 8)])


```

```{r, echo = FALSE}
perm = permutations(n = 4, r = 4, v = 1:4)   
gamma_minus_label = vector(length = 24)
for(i in 1:24){
  gamma_minus_label[i] = sum(abs(out_mu_4_6$gammas[,perm[i,]] - newsgroups.onehot))
}
```

Looking again at the value we get for the minimum difference we get between the gammas and the actual labels (divided by number of rows of data)
```{r, echo = FALSE}
min(gamma_minus_label)/nrow(documents)
```














The original function selects equal starting weights to each cluster written as:  `rep(log(1/K),K)`. Here the function is using logs to keep the numbers stable.
\

**(B)** I will now run the algorithm with starting weights equal to the proportion of each type of article.

```{r, include=FALSE}

# Calculating proportion of each type of article

sum(newsgroups == 1)/length(newsgroups)
sum(newsgroups == 2)/length(newsgroups)
sum(newsgroups == 3)/length(newsgroups)
sum(newsgroups == 4)/length(newsgroups)


top8_words_W_prop = data.frame('1' = wordlist[which.maxn(out_W_prop$mus[1,], n = 8)],
                               '2' = wordlist[which.maxn(out_W_prop$mus[2,], n = 8)],
                               '3' = wordlist[which.maxn(out_W_prop$mus[3,], n = 8)],
                               '4' = wordlist[which.maxn(out_W_prop$mus[4,], n = 8)])

```


```{r, echo = FALSE}
perm = permutations(n = 4, r = 4, v = 1:4)   
gamma_minus_label = vector(length = 24)
for(i in 1:24){
  gamma_minus_label[i] = sum(abs(out_W_prop$gammas[,perm[i,]] - newsgroups.onehot))
}
```

Using the same measure:
```{r, echo = FALSE}
min(gamma_minus_label)/nrow(documents)
```






**(C)** I will now run the algorithm with starting weights equal to the proportion of each type of article and starting $\mu's$ equal to the proportion of times a word appeared and adding 0.01 so that we have no zero values.

```{r, include = FALSE, eval=FALSE}
# proportion of times a word appeared and adding 0.01 so that we have no zero values.

i = 1:100

mus_prop = matrix(c(
colSums(documents[newsgroups == 1,])/(sum(newsgroups == 1)),
colSums(documents[newsgroups == 2,])/(sum(newsgroups == 2)),
colSums(documents[newsgroups == 3,])/(sum(newsgroups == 3)),
colSums(documents[newsgroups == 4,])/(sum(newsgroups == 4))
), nrow = 4, ncol = 100)

mus_prop = mus_prop + 0.01
```

```{r, include=FALSE}
top8_words_W_mu_prop = data.frame('1' = wordlist[which.maxn(out_W_mu_prop$mus[1,], n = 8)],
                               '2' = wordlist[which.maxn(out_W_mu_prop$mus[2,], n = 8)],
                               '3' = wordlist[which.maxn(out_W_mu_prop$mus[3,], n = 8)],
                               '4' = wordlist[which.maxn(out_W_mu_prop$mus[4,], n = 8)])

```


```{r, echo = FALSE}
perm = permutations(n = 4, r = 4, v = 1:4)   
gamma_minus_label = vector(length = 24)
for(i in 1:24){
  gamma_minus_label[i] = sum(abs(out_W_mu_prop$gammas[,perm[i,]] - newsgroups.onehot))
}
```

Using the same measure:
```{r, echo = FALSE}
min(gamma_minus_label)/nrow(documents)
```

Below I have made the same tables as before. The first row has the Table for case A on the left and then for B on the right. Then at the bottom is the table for case C. 

```{r, echo = FALSE}
kable(cbind(top8_words_mu_4_6, top8_words_W_prop), col.names = cbind(1:4,1:4)) 

```

```{r, echo = FALSE}
kable(top8_words_W_mu_prop,  col.names = 1:4) 
 
```

So in conclusion we can see from the tables that the clusters that seem to represent postings about computers and sport are very stable, but the other two less so.
\
And looking at the measure I used to determine 'accuracy', the output didn't seem to be too sensitive to changes in starting values. It would of been best if I had ran each case a couple of times, as we do get different output anyways due to the randomness within the functions I used e.g. the functions use `sample`.
